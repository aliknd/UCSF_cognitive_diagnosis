{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb3aba43-4ad0-4fa9-9158-996c75ce1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 0. Imports & paths =========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# modelling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score, classification_report\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from imblearn.pipeline import Pipeline        # pip install imbalanced-learn\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "DATA_DIR = Path(\"./\")                         # adjust if needed\n",
    "HR_FILE  = DATA_DIR / \"heartrate_15min.csv\"\n",
    "DX_FILE  = DATA_DIR / \"Diagnoses_20250404.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f01e12-f644-46f8-94c3-53366909bcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR rows: 506496 | participants: 192\n"
     ]
    }
   ],
   "source": [
    "# ========== 1. Load tables & attach BaselineDate ======================\n",
    "# --- diagnoses --------------------------------------------------------\n",
    "diag = (pd.read_csv(DX_FILE, parse_dates=[\"DCDate.diagnosis_baseline\"])\n",
    "          .rename(columns={\"DCDate.diagnosis_baseline\": \"BaselineDate\"}))\n",
    "\n",
    "# keep only rows that HAVE a baseline date\n",
    "diag = diag.dropna(subset=[\"BaselineDate\"])\n",
    "diag = diag[[\"PIDN\", \"BaselineDate\", \"Diagnosis_baseline_3groups\"]]\n",
    "\n",
    "# --- heart-rate -------------------------------------------------------\n",
    "hr = pd.read_csv(HR_FILE, parse_dates=[\"Time\"])\n",
    "\n",
    "# --- intersect on PIDN -----------------------------------------------\n",
    "common_pidn = set(diag.PIDN) & set(hr.PIDN)\n",
    "diag = diag[diag.PIDN.isin(common_pidn)].copy()\n",
    "hr   = hr [hr .PIDN.isin(common_pidn)].copy()\n",
    "\n",
    "# attach each participant's baseline date to every HR row\n",
    "hr = hr.merge(diag[[\"PIDN\", \"BaselineDate\"]], on=\"PIDN\", how=\"left\")\n",
    "assert hr[\"BaselineDate\"].notna().all()\n",
    "\n",
    "print(\"HR rows:\", len(hr), \"| participants:\", hr.PIDN.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8c48d6b-701e-43a1-a327-68d291f15c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7-day HR rows: 100157 | participants: 192\n"
     ]
    }
   ],
   "source": [
    "# ========== 2. Build the 7-day HR window ==============================\n",
    "def first_n_days(grp: pd.DataFrame, baseline_date, n=7):\n",
    "    \"\"\"Return rows for the first n calendar days on/after baseline_date.\"\"\"\n",
    "    after = grp[grp.Time.dt.date >= baseline_date]\n",
    "    start = after.Time.dt.date.min() if not after.empty else grp.Time.dt.date.min()\n",
    "    end   = start + pd.Timedelta(days=n)        # exclusive upper bound\n",
    "    return grp[(grp.Time.dt.date >= start) & (grp.Time.dt.date < end)]\n",
    "\n",
    "hr7_slices = []\n",
    "for pid, g in hr.groupby(\"PIDN\"):\n",
    "    bdate = g[\"BaselineDate\"].iloc[0].date()\n",
    "    win   = first_n_days(g, bdate, n=7)\n",
    "    if not win.empty:\n",
    "        hr7_slices.append(win)\n",
    "\n",
    "hr7 = pd.concat(hr7_slices, ignore_index=True)\n",
    "print(\"7-day HR rows:\", len(hr7), \"| participants:\", hr7.PIDN.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f63ddd-223f-46a7-a168-f02f1461ff3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved hr7day_features.csv | shape = (192, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PIDN</th>\n",
       "      <th>day_mean</th>\n",
       "      <th>hr_iqr</th>\n",
       "      <th>hr_max</th>\n",
       "      <th>hr_mean</th>\n",
       "      <th>hr_median</th>\n",
       "      <th>hr_min</th>\n",
       "      <th>hr_p10</th>\n",
       "      <th>hr_p90</th>\n",
       "      <th>hr_std</th>\n",
       "      <th>night_mean</th>\n",
       "      <th>rmssd</th>\n",
       "      <th>tachy_prop</th>\n",
       "      <th>Diagnosis_baseline_3groups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1416</td>\n",
       "      <td>72.265306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>72.094340</td>\n",
       "      <td>70.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>7.836210</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>8.234543</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>Clinically Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2502</td>\n",
       "      <td>75.539007</td>\n",
       "      <td>16.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>71.638844</td>\n",
       "      <td>69.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>11.350094</td>\n",
       "      <td>63.390000</td>\n",
       "      <td>5.582903</td>\n",
       "      <td>0.014446</td>\n",
       "      <td>Clinically Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2692</td>\n",
       "      <td>87.738506</td>\n",
       "      <td>14.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>86.332506</td>\n",
       "      <td>86.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10.650021</td>\n",
       "      <td>77.436364</td>\n",
       "      <td>8.258419</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>Clinically Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3700</td>\n",
       "      <td>68.200456</td>\n",
       "      <td>9.5</td>\n",
       "      <td>91.0</td>\n",
       "      <td>67.763198</td>\n",
       "      <td>67.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>6.980453</td>\n",
       "      <td>66.906250</td>\n",
       "      <td>4.622126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Clinically Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3835</td>\n",
       "      <td>76.830357</td>\n",
       "      <td>18.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>71.933036</td>\n",
       "      <td>69.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>15.521106</td>\n",
       "      <td>62.138393</td>\n",
       "      <td>8.486599</td>\n",
       "      <td>0.080357</td>\n",
       "      <td>Clinically Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PIDN   day_mean  hr_iqr  hr_max    hr_mean  hr_median  hr_min  hr_p10  \\\n",
       "0  1416  72.265306     0.0   115.0  72.094340       70.0    67.0    70.0   \n",
       "1  2502  75.539007    16.0   109.0  71.638844       69.0    53.0    59.0   \n",
       "2  2692  87.738506    14.0   127.0  86.332506       86.0    64.0    71.0   \n",
       "3  3700  68.200456     9.5    91.0  67.763198       67.0    53.0    59.0   \n",
       "4  3835  76.830357    18.0   120.0  71.933036       69.0    50.0    55.0   \n",
       "\n",
       "   hr_p90     hr_std  night_mean     rmssd  tachy_prop  \\\n",
       "0    74.0   7.836210   70.000000  8.234543    0.037736   \n",
       "1    88.0  11.350094   63.390000  5.582903    0.014446   \n",
       "2   100.0  10.650021   77.436364  8.258419    0.096774   \n",
       "3    77.0   6.980453   66.906250  4.622126    0.000000   \n",
       "4    98.0  15.521106   62.138393  8.486599    0.080357   \n",
       "\n",
       "  Diagnosis_baseline_3groups  \n",
       "0          Clinically Normal  \n",
       "1          Clinically Normal  \n",
       "2          Clinically Normal  \n",
       "3          Clinically Normal  \n",
       "4          Clinically Normal  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------- STEP 3 : 7-day HR feature table ----------------------\n",
    "# (run after Steps 1–2 so `hr7` and `diag` are defined)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def hr_features(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Summary stats across the participant’s 7-day HR window.\"\"\"\n",
    "    v = df[\"Value\"].to_numpy()\n",
    "    n = v.size\n",
    "    hrs        = df[\"Time\"].dt.hour\n",
    "    day_mask   = hrs.between(6, 21)\n",
    "    night_mask = ~day_mask\n",
    "    mean_ = lambda a: np.nan if a.size == 0 else a.mean()\n",
    "    pct_  = lambda cond: np.nan if n == 0 else cond.mean()\n",
    "\n",
    "    return pd.Series({\n",
    "        \"hr_mean\"   : mean_(v),\n",
    "        \"hr_median\" : np.nan if n == 0 else np.median(v),\n",
    "        \"hr_std\"    : np.nan if n == 0 else np.std(v, ddof=0),\n",
    "        \"hr_min\"    : np.nan if n == 0 else v.min(),\n",
    "        \"hr_max\"    : np.nan if n == 0 else v.max(),\n",
    "        \"hr_iqr\"    : np.nan if n == 0 else np.percentile(v, 75) - np.percentile(v, 25),\n",
    "        \"hr_p10\"    : np.nan if n == 0 else np.percentile(v, 10),\n",
    "        \"hr_p90\"    : np.nan if n == 0 else np.percentile(v, 90),\n",
    "        \"tachy_prop\": pct_(v > 100),\n",
    "        \"rmssd\"     : np.nan if n < 2 else np.sqrt(np.mean(np.diff(v)**2)),\n",
    "        \"day_mean\"  : mean_(df.loc[day_mask,   \"Value\"].to_numpy()),\n",
    "        \"night_mean\": mean_(df.loc[night_mask, \"Value\"].to_numpy())\n",
    "    })\n",
    "\n",
    "# --- build one row per participant ------------------------------------\n",
    "features7 = (\n",
    "    hr7.groupby(\"PIDN\")[[\"Time\", \"Value\"]]     # only the cols we need → no warning\n",
    "       .apply(hr_features)                    # apply summariser\n",
    "       .reset_index()                         # PIDN becomes a normal column\n",
    ")\n",
    "\n",
    "# drop any duplicated columns that might sneak in\n",
    "features7 = features7.loc[:, ~features7.columns.duplicated()]\n",
    "\n",
    "# keep numeric feature columns + PIDN\n",
    "numeric_cols = features7.select_dtypes(include=\"number\").columns.difference([\"PIDN\"])\n",
    "features7    = features7[[\"PIDN\"] + numeric_cols.tolist()]\n",
    "\n",
    "# attach 3-group diagnosis\n",
    "data7 = features7.merge(\n",
    "    diag[[\"PIDN\", \"Diagnosis_baseline_3groups\"]],\n",
    "    on=\"PIDN\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# save for convenience\n",
    "data7.to_csv(\"hr7day_features.csv\", index=False)\n",
    "print(\"Saved hr7day_features.csv | shape =\", data7.shape)\n",
    "\n",
    "# quick look\n",
    "data7.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7be25df-83ac-4860-88e3-4db3eccd4476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance: {0: 122, 1: 70}\n",
      "\n",
      "TEST balanced-accuracy: 0.439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CN       0.59      0.52      0.55        25\n",
      "    Abnormal       0.29      0.36      0.32        14\n",
      "\n",
      "    accuracy                           0.46        39\n",
      "   macro avg       0.44      0.44      0.44        39\n",
      "weighted avg       0.48      0.46      0.47        39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Binary sanity-check:  Clinically Normal (0)  vs  Abnormal (1)\n",
    "# ================================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ----- 1.  X / y  -------------------------------------------------\n",
    "X = data7.drop(columns=[\"Diagnosis_baseline_3groups\", \"PIDN\"])\n",
    "y = (data7[\"Diagnosis_baseline_3groups\"] != \"Clinically Normal\").astype(int)   # 1 = MCI/AD or FTD\n",
    "\n",
    "print(\"Class balance:\", y.value_counts().to_dict())   # sanity-check\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# ----- 2.  pipeline (no oversampling) -----------------------------\n",
    "pipe_bin = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\" , StandardScaler()),\n",
    "    (\"model\" , HistGradientBoostingClassifier(\n",
    "                  random_state=RANDOM_STATE,\n",
    "                  class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "pipe_bin.fit(X_train, y_train)\n",
    "\n",
    "# ----- 3.  evaluation --------------------------------------------\n",
    "y_pred = pipe_bin.predict(X_test)\n",
    "print(\"\\nTEST balanced-accuracy:\",\n",
    "      round(balanced_accuracy_score(y_test, y_pred), 3))\n",
    "print(classification_report(y_test, y_pred, target_names=[\"CN\", \"Abnormal\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa7639d-bffc-429a-826c-2051457c28c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_FILE = Path(\"minuteStepsNarrow.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46b23b-578b-4450-ac43-e33342a0818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------  S-1  read & filter large steps CSV  --------------------\n",
    "use_pids = set(features7.PIDN)            # the 192 participants we kept after HR step\n",
    "\n",
    "chunks = []\n",
    "for chunk in pd.read_csv(\n",
    "        ST_FILE,\n",
    "        usecols=[\"PIDN\", \"ActivityMinute\", \"Steps\"],   # skip Fitbit model column\n",
    "        parse_dates=[\"ActivityMinute\"],\n",
    "        dtype={\"PIDN\": \"int32\", \"Steps\": \"int32\"},\n",
    "        chunksize=5_000_000,          # ~100-150 MB per chunk\n",
    "        low_memory=True):\n",
    "    \n",
    "    filt = chunk[chunk[\"PIDN\"].isin(use_pids)]\n",
    "    if not filt.empty:\n",
    "        chunks.append(filt)\n",
    "\n",
    "steps_raw = pd.concat(chunks, ignore_index=True)\n",
    "steps_raw = steps_raw.rename(columns={\"ActivityMinute\": \"Time\", \"Steps\": \"Value\"})\n",
    "print(\"Minute-steps rows kept:\", len(steps_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad847cc0-0aac-41fd-b3ee-cf42943ed01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach BaselineDate to steps rows\n",
    "steps_raw = steps_raw.merge(diag[[\"PIDN\", \"BaselineDate\"]], on=\"PIDN\", how=\"left\")\n",
    "assert steps_raw[\"BaselineDate\"].notna().all()\n",
    "\n",
    "step7 = pd.concat(\n",
    "    [first_n_days(g, g[\"BaselineDate\"].iloc[0].date(), n=7)\n",
    "     for pid, g in steps_raw.groupby(\"PIDN\") if not g.empty],\n",
    "    ignore_index=True\n",
    ")\n",
    "print(\"Minute-steps rows in 7-day window:\", len(step7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee330d3-8c4b-48bc-96b7-1d203a9c729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------  S-3  resample 15-min & build step features  -------------\n",
    "def build_step_features(grp: pd.DataFrame) -> pd.Series:\n",
    "    # 1. resample to 15-min bins, summing steps within each bin\n",
    "    ts = (grp.set_index(\"Time\")\n",
    "              .sort_index()\n",
    "              [\"Value\"]\n",
    "              .resample(\"15min\")          # ← updated keyword\n",
    "              .sum(min_count=1)\n",
    "              .ffill(limit=1))            # carry forward a single missing bin\n",
    "\n",
    "    v = ts.to_numpy()\n",
    "    n = v.size\n",
    "    hrs = ts.index.hour                  # Int64Index\n",
    "\n",
    "    # day = 06:00-21:59  (bool mask)\n",
    "    day_mask = (hrs >= 6) & (hrs <= 21)  # ← no .between() needed\n",
    "\n",
    "    mean_ = lambda a: np.nan if a.size == 0 else a.mean()\n",
    "\n",
    "    return pd.Series({\n",
    "        \"steps_mean\"      : mean_(v),\n",
    "        \"steps_std\"       : np.std(v, ddof=0),\n",
    "        \"steps_max\"       : v.max(),\n",
    "        \"steps_iqr\"       : np.percentile(v, 75) - np.percentile(v, 25),\n",
    "        \"steps_p10\"       : np.percentile(v, 10),\n",
    "        \"steps_p90\"       : np.percentile(v, 90),\n",
    "        \"sedentary_prop\"  : (v == 0).mean(),\n",
    "        \"moderate_bouts\"  : (v >= 100).sum(),   # 100+ steps / 15 min\n",
    "        \"vigorous_bouts\"  : (v >= 250).sum(),   # 250+ steps / 15 min\n",
    "        \"day_steps_mean\"  : mean_(v[day_mask]),\n",
    "        \"night_steps_mean\": mean_(v[~day_mask]),\n",
    "    })\n",
    "\n",
    "step_feat = (\n",
    "    step7.groupby(\"PIDN\")[[\"Time\", \"Value\"]]\n",
    "         .apply(build_step_features)\n",
    "         .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Step-feature table shape:\", step_feat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b287fbb-bd77-4cb3-82ff-6e9f4a801ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with existing HR feature table\n",
    "full_features = (features7\n",
    "                 .merge(step_feat, on=\"PIDN\", how=\"left\")      # may introduce NaNs\n",
    "                 .merge(diag[[\"PIDN\", \"Diagnosis_baseline_3groups\"]], on=\"PIDN\"))\n",
    "\n",
    "full_features.to_csv(\"hr_steps_7day_features.csv\", index=False)\n",
    "print(\"Merged HR+Steps feature table  |  shape =\", full_features.shape)\n",
    "\n",
    "# ---------- ready for modelling -----------------\n",
    "X = full_features.drop(columns=[\"Diagnosis_baseline_3groups\", \"PIDN\"])\n",
    "y = full_features[\"Diagnosis_baseline_3groups\"]\n",
    "\n",
    "# same pipeline you used before\n",
    "pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\" , StandardScaler()),\n",
    "    (\"adasyn\", ADASYN(random_state=RANDOM_STATE, sampling_strategy=\"auto\")),\n",
    "    (\"model\" , HistGradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"model__learning_rate\": [0.05, 0.1],\n",
    "    \"model__max_depth\"    : [None, 3]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    pipe, param_grid, cv=cv,\n",
    "    scoring=make_scorer(balanced_accuracy_score),\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "gs.fit(X, y)\n",
    "\n",
    "print(\"\\nBest CV balanced-accuracy:\", round(gs.best_score_, 3))\n",
    "print(\"Best params:\", gs.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0310c47-7896-49d1-857b-cb1738aeacaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Modelling cell – median-impute ➜ scale ➜ HistGB (no ADASYN)\n",
    "# ---------------------------------------------------------------\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, make_scorer\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- train / test split (reuse if you still have X_train, X_test) -----\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# --- pipeline without synthetic oversampling --------------------------\n",
    "pipe_no_synth = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\" , StandardScaler()),\n",
    "    (\"model\" , HistGradientBoostingClassifier(\n",
    "                  random_state=RANDOM_STATE,\n",
    "                  class_weight=\"balanced\"))      # handles imbalance internally\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"model__learning_rate\": [0.05, 0.1],\n",
    "    \"model__max_depth\"    : [None, 3]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    pipe_no_synth, param_grid, cv=cv,\n",
    "    scoring=make_scorer(balanced_accuracy_score),\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest CV balanced-accuracy:\", round(gs.best_score_, 3))\n",
    "print(\"Best parameters:\", gs.best_params_)\n",
    "\n",
    "# ---- evaluate on held-out test set -----------------------------------\n",
    "y_pred = gs.predict(X_test)\n",
    "print(\"\\nTEST balanced-accuracy:\", round(balanced_accuracy_score(y_test, y_pred), 3))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab77212-df81-4567-ad8f-f38659352dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
