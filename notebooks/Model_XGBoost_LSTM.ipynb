{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94856d33-93c8-476a-b6d1-5170a545aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost lightgbm imbalanced-learn tensorflow==2.15 --quiet\n",
    "import numpy as np, pandas as pd, joblib, os, gc, warnings, random, math\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "# paths\n",
    "DATA_DIR   = Path(\".\")\n",
    "HR_FILE    = DATA_DIR / \"heartrate_15min.csv\"\n",
    "STEPS_FILE = DATA_DIR / \"minuteStepsNarrow.csv\"\n",
    "DX_FILE    = DATA_DIR / \"Diagnoses_20250404.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9344c09-8fb2-4c40-8afd-690fd6781d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 17) tabular rows saved\n"
     ]
    }
   ],
   "source": [
    "# ---------- load diagnosis ------------------------------------------------\n",
    "diag = (pd.read_csv(DX_FILE, parse_dates=[\"DCDate.diagnosis_baseline\"])\n",
    "          .rename(columns={\"DCDate.diagnosis_baseline\":\"BaselineDate\"})\n",
    "          .dropna(subset=[\"BaselineDate\"])\n",
    "          [[\"PIDN\",\"BaselineDate\",\"Diagnosis_baseline_3groups\"]])\n",
    "\n",
    "# ---------- helper to slice first N days ----------------------------------\n",
    "def slice_days(df, base_date, n=14):\n",
    "    after = df[df.Time.dt.date >= base_date]\n",
    "    start = after.Time.min() if not after.empty else df.Time.min()\n",
    "    end   = start + pd.Timedelta(days=n)\n",
    "    return df[(df.Time >= start) & (df.Time < end)]\n",
    "\n",
    "# ---------- HEART RATE ----------------------------------------------------\n",
    "hr = pd.read_csv(HR_FILE, parse_dates=[\"Time\"]).merge(diag, \"inner\", \"PIDN\")\n",
    "hr14 = pd.concat([slice_days(g, g.BaselineDate.iloc[0].date(), 14)\n",
    "                  for _, g in hr.groupby(\"PIDN\")])\n",
    "\n",
    "def hr_stats(df):\n",
    "    v = df.Value.to_numpy()\n",
    "    h = df.Time.dt.hour\n",
    "    day, night = h.between(6,21), ~h.between(6,21)\n",
    "    rmssd = np.sqrt(np.mean(np.diff(v)**2)) if v.size>1 else np.nan\n",
    "    sdnn  = np.std(v, ddof=0)\n",
    "    return pd.Series({\n",
    "        \"hr_mean\":v.mean(), \"hr_std\":sdnn, \"hr_min\":v.min(), \"hr_max\":v.max(),\n",
    "        \"rmssd\":rmssd, \"lfhf\":rmssd/(sdnn+1e-6),\n",
    "        \"day_mean\":df.Value[day].mean(), \"night_mean\":df.Value[night].mean()\n",
    "    })\n",
    "feat_hr = hr14.groupby(\"PIDN\").apply(hr_stats).reset_index()\n",
    "\n",
    "# ---------- minute steps → daily totals -----------------------------------\n",
    "steps_raw = (pd.read_csv(STEPS_FILE, parse_dates=[\"ActivityMinute\"])\n",
    "               .rename(columns={\"ActivityMinute\":\"Time\",\"Steps\":\"Value\"})\n",
    "               .merge(diag[[\"PIDN\",\"BaselineDate\"]],\"inner\"))\n",
    "step14 = pd.concat([slice_days(g,g.BaselineDate.iloc[0].date(),14)\n",
    "                    for _,g in steps_raw.groupby(\"PIDN\")])\n",
    "\n",
    "step_daily = (step14.assign(Date=step14.Time.dt.date)\n",
    "                      .groupby([\"PIDN\",\"Date\"]).Value.sum()\n",
    "                      .rename(\"steps\").reset_index())\n",
    "\n",
    "def st_stats(df):\n",
    "    v=df.steps.to_numpy()\n",
    "    trend=v[-1]-v[0] if len(v)>1 else np.nan\n",
    "    wknd_mask = pd.to_datetime(df.Date).dt.dayofweek>=5\n",
    "    return pd.Series({\n",
    "        \"steps_mean\":v.mean(),\"steps_std\":v.std(ddof=0),\n",
    "        \"steps_min\":v.min(),\"steps_max\":v.max(),\n",
    "        \"steps_trend\":trend,\n",
    "        \"wknd_ratio\":v[wknd_mask].mean()/(v[~wknd_mask].mean()+1e-6)\n",
    "    })\n",
    "feat_st = step_daily.groupby(\"PIDN\").apply(st_stats).reset_index()\n",
    "\n",
    "# ---------- HR–steps coupling --------------------------------------------\n",
    "hr_daily = (hr14.assign(Date=hr14.Time.dt.date)\n",
    "                 .groupby([\"PIDN\",\"Date\"]).Value.mean()\n",
    "                 .rename(\"hr\").reset_index())\n",
    "coupling = (hr_daily.merge(step_daily,\"inner\")\n",
    "                   .groupby(\"PIDN\")\n",
    "                   .apply(lambda d: pd.Series({\"hr_steps_corr\":\n",
    "                         np.corrcoef(d.hr,d.steps)[0,1] if len(d)>2 else np.nan}))\n",
    "                   .reset_index())\n",
    "\n",
    "# ---------- merge all features & label -----------------------------------\n",
    "tab = (feat_hr.merge(feat_st,\"left\").merge(coupling,\"left\")\n",
    "       .merge(diag[[\"PIDN\",\"Diagnosis_baseline_3groups\"]],\"inner\"))\n",
    "tab.to_csv(\"tabular_features.csv\",index=False)\n",
    "print(tab.shape, \"tabular rows saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fc502c2-ba5e-444f-820e-3d8ba610c9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB OOF balanced-acc: 0.534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgb_full.joblib']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "X = tab.drop(columns=[\"Diagnosis_baseline_3groups\",\"PIDN\"])\n",
    "y = (tab.Diagnosis_baseline_3groups!=\"Clinically Normal\").astype(int).to_numpy()\n",
    "\n",
    "kf = StratifiedKFold(10, shuffle=True, random_state=RANDOM_STATE)\n",
    "p_tab = np.zeros_like(y, dtype=float)\n",
    "\n",
    "for train_idx, val_idx in kf.split(X,y):\n",
    "    dtrain = xgb.DMatrix(X.iloc[train_idx], label=y[train_idx])\n",
    "    dval   = xgb.DMatrix(X.iloc[val_idx])\n",
    "    bst = xgb.train(\n",
    "        params=dict(max_depth=4, learning_rate=0.07,\n",
    "                    subsample=0.8, colsample_bytree=0.8,\n",
    "                    objective=\"binary:logistic\", eval_metric=\"logloss\",\n",
    "                    random_state=RANDOM_STATE),\n",
    "        dtrain=dtrain, num_boost_round=200, verbose_eval=False)\n",
    "    p_tab[val_idx] = bst.predict(dval)\n",
    "\n",
    "print(\"XGB OOF balanced-acc:\",\n",
    "      balanced_accuracy_score(y, (p_tab>=0.5).astype(int)).round(3))\n",
    "joblib.dump(bst,\"xgb_full.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d672ef6f-729e-490e-a0bd-a8b255f80280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq tensor: (192, 1344, 2)\n"
     ]
    }
   ],
   "source": [
    "# down-sample minute steps to 15-min bins, align with HR index\n",
    "# reuse hr14 & step14 from CELL 1\n",
    "def to_channel(df, full_index, sentinel):\n",
    "    s = df.reindex(full_index)[\"Value\"].astype(float)\n",
    "    mu, sd = s.mean(), s.std(ddof=0)\n",
    "    return s.sub(mu).div(sd+1e-6).fillna(sentinel).to_numpy()\n",
    "\n",
    "sentinel = -1000.0\n",
    "seqs = []\n",
    "for pid in tab.PIDN:\n",
    "    idx = hr14.Time[hr14.PIDN==pid].min().floor(\"D\")\n",
    "    full_idx = pd.date_range(idx, periods=96*14, freq=\"15min\")\n",
    "    ch_hr = to_channel(hr14[hr14.PIDN==pid].set_index(\"Time\"), full_idx, sentinel)\n",
    "    st_15 = (step14[step14.PIDN == pid]\n",
    "               .set_index(\"Time\").Value\n",
    "               .resample(\"15min\").sum()\n",
    "               .to_frame(name=\"Value\"))\n",
    "    ch_st = to_channel(st_15, full_idx, sentinel)\n",
    "    seqs.append(np.stack([ch_hr, ch_st], axis=-1))    # shape 1344×2\n",
    "X_seq = np.stack(seqs)\n",
    "np.save(\"seq.npy\",X_seq); np.save(\"labels.npy\",y)\n",
    "print(\"Seq tensor:\",X_seq.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ab86ee4-aab9-45d7-85cd-af539cc0626d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000027992F2AA20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002799D557420> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU OOF balanced-acc: 0.682\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight   # NEW\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "tf.keras.utils.set_random_seed(RANDOM_STATE)\n",
    "\n",
    "def build_seq_model(input_shape, sentinel):\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.Masking(mask_value=sentinel)(inp)\n",
    "    x = layers.Bidirectional(layers.GRU(32, return_sequences=True,\n",
    "                                        dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    x = layers.MultiHeadAttention(2, 16)(x,x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    mdl = Model(inp, out)\n",
    "    mdl.compile(tf.keras.optimizers.Adam(1e-3),\n",
    "                loss=\"binary_crossentropy\")\n",
    "    return mdl\n",
    "\n",
    "p_seq = np.zeros_like(y, dtype=float)\n",
    "kf = StratifiedKFold(10, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "for fold,(tr,vl) in enumerate(kf.split(X_seq,y)):\n",
    "    model = build_seq_model((1344,2), sentinel)\n",
    "    cw = compute_class_weight(\"balanced\",classes=np.array([0,1]),y=y[tr])\n",
    "    model.fit(X_seq[tr], y[tr],\n",
    "              epochs=50, batch_size=32, verbose=0,\n",
    "              validation_data=(X_seq[vl], y[vl]),\n",
    "              class_weight={0:cw[0],1:cw[1]},\n",
    "              callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "                  patience=8, restore_best_weights=True, monitor=\"val_loss\")])\n",
    "    p_seq[vl] = model.predict(X_seq[vl], verbose=0).ravel()\n",
    "\n",
    "print(\"GRU OOF balanced-acc:\",\n",
    "      balanced_accuracy_score(y, (p_seq>=0.5).astype(int)).round(3))\n",
    "model.save(\"gru_full.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a2e9dff-79ae-4a59-bd7f-9bd9a9ac8c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blender OOF balanced-acc: 0.672\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "meta = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "meta_X = np.column_stack([p_tab, p_seq])\n",
    "meta.fit(meta_X, y)\n",
    "joblib.dump(meta,\"blender.joblib\")\n",
    "\n",
    "print(\"Blender OOF balanced-acc:\",\n",
    "      balanced_accuracy_score(y, meta.predict(meta_X)).round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9674fa5a-40b8-4f78-b390-e7f71055df91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST balanced-accuracy: 0.577\n",
      "Abnormal recall: 0.714\n",
      "[[11 14]\n",
      " [ 4 10]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb, numpy as np, tensorflow as tf, joblib\n",
    "from sklearn.metrics import balanced_accuracy_score, recall_score, confusion_matrix\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0.  train/test split (same random_state as before)\n",
    "# ------------------------------------------------------------\n",
    "X_tab_train, X_tab_test, y_train, y_test, \\\n",
    "X_seq_train, X_seq_test = train_test_split(\n",
    "    tab.drop(columns=[\"Diagnosis_baseline_3groups\",\"PIDN\"]),\n",
    "    y, X_seq,\n",
    "    test_size=0.20, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.  retrain XGBoost on the *training* fold\n",
    "# ------------------------------------------------------------\n",
    "params_xgb = dict(\n",
    "    max_depth=4,\n",
    "    learning_rate=0.07,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "num_rounds = 200                      # same as Cell 2\n",
    "\n",
    "dtrain = xgb.DMatrix(X_tab_train, label=y_train)\n",
    "dtest  = xgb.DMatrix(X_tab_test)\n",
    "bst_final = xgb.train(params_xgb, dtrain, num_boost_round=num_rounds)\n",
    "probs_tab = bst_final.predict(dtest)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  retrain GRU-Attention on the *training* fold\n",
    "# ------------------------------------------------------------\n",
    "gru_final = build_seq_model((1344, 2), sentinel)\n",
    "cw = compute_class_weight(\"balanced\", classes=np.array([0, 1]), y=y_train)\n",
    "gru_final.fit(\n",
    "    X_seq_train, y_train,\n",
    "    epochs=50, batch_size=32, verbose=0,\n",
    "    validation_split=0.15,\n",
    "    class_weight={0: cw[0], 1: cw[1]},\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "        patience=8, restore_best_weights=True, monitor=\"val_loss\")]\n",
    ")\n",
    "probs_seq = gru_final.predict(X_seq_test, verbose=0).ravel()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.  blend with the logistic meta-model trained in Cell 5\n",
    "# ------------------------------------------------------------\n",
    "meta   = joblib.load(\"blender.joblib\")\n",
    "probs_blend = meta.predict_proba(\n",
    "    np.column_stack([probs_tab, probs_seq])\n",
    ")[:, 1]\n",
    "y_pred = (probs_blend >= 0.5).astype(int)\n",
    "\n",
    "print(\"TEST balanced-accuracy:\",\n",
    "      round(balanced_accuracy_score(y_test, y_pred), 3))\n",
    "print(\"Abnormal recall:\",\n",
    "      round(recall_score(y_test, y_pred), 3))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89ecc666-93e9-4434-9068-6be49442f3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cut</th>\n",
       "      <th>bal_acc</th>\n",
       "      <th>abn_recall</th>\n",
       "      <th>false_pos</th>\n",
       "      <th>false_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.612857</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.612857</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.612857</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.612857</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.612857</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.612857</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.612857</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.612857</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.612857</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.612857</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.612857</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.612857</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.612857</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.577143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.577143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.577143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.577143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.577143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.577143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.577143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.597143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.617143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.581429</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.581429</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.581429</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.581429</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cut   bal_acc  abn_recall  false_pos  false_neg\n",
       "0   0.35  0.612857    0.785714         14          3\n",
       "1   0.36  0.612857    0.785714         14          3\n",
       "2   0.37  0.612857    0.785714         14          3\n",
       "3   0.38  0.612857    0.785714         14          3\n",
       "4   0.39  0.612857    0.785714         14          3\n",
       "5   0.40  0.612857    0.785714         14          3\n",
       "6   0.41  0.612857    0.785714         14          3\n",
       "7   0.42  0.612857    0.785714         14          3\n",
       "8   0.43  0.612857    0.785714         14          3\n",
       "9   0.44  0.612857    0.785714         14          3\n",
       "10  0.45  0.612857    0.785714         14          3\n",
       "11  0.46  0.612857    0.785714         14          3\n",
       "12  0.47  0.612857    0.785714         14          3\n",
       "13  0.48  0.577143    0.714286         14          4\n",
       "14  0.49  0.577143    0.714286         14          4\n",
       "15  0.50  0.577143    0.714286         14          4\n",
       "16  0.51  0.577143    0.714286         14          4\n",
       "17  0.52  0.577143    0.714286         14          4\n",
       "18  0.53  0.577143    0.714286         14          4\n",
       "19  0.54  0.577143    0.714286         14          4\n",
       "20  0.55  0.597143    0.714286         13          4\n",
       "21  0.56  0.617143    0.714286         12          4\n",
       "22  0.57  0.581429    0.642857         12          5\n",
       "23  0.58  0.581429    0.642857         12          5\n",
       "24  0.59  0.581429    0.642857         12          5\n",
       "25  0.60  0.581429    0.642857         12          5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best balanced-accuracy : {'cut': 0.5600000000000002, 'bal_acc': 0.6171428571428572, 'abn_recall': 0.7142857142857143, 'false_pos': 12.0, 'false_neg': 4.0}\n",
      "≥70 % abnormal recall : {'cut': 0.35, 'bal_acc': 0.6128571428571429, 'abn_recall': 0.7857142857142857, 'false_pos': 14.0, 'false_neg': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Threshold scan for blended model\n",
    "# ================================================================\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.metrics import balanced_accuracy_score, recall_score, confusion_matrix\n",
    "\n",
    "scan = np.arange(0.35, 0.61, 0.01)          # cut-off range to explore\n",
    "rows = []\n",
    "for cut in scan:\n",
    "    y_pred = (probs_blend >= cut).astype(int)\n",
    "    ba   = balanced_accuracy_score(y_test, y_pred)\n",
    "    rec1 = recall_score(y_test, y_pred)           # abnormal recall\n",
    "    fp   = ((y_pred == 1) & (y_test == 0)).sum()\n",
    "    fn   = ((y_pred == 0) & (y_test == 1)).sum()\n",
    "    rows.append([cut, ba, rec1, fp, fn])\n",
    "\n",
    "thr_df = pd.DataFrame(rows,\n",
    "                      columns=[\"cut\", \"bal_acc\", \"abn_recall\", \"false_pos\", \"false_neg\"])\n",
    "display(thr_df)\n",
    "\n",
    "# --- best rows -----------------------------------------------------------\n",
    "best_row   = thr_df.loc[thr_df[\"bal_acc\"].idxmax()]\n",
    "rec70_row  = thr_df[thr_df[\"abn_recall\"] >= 0.70].head(1)   # first cut hitting ≥70 % recall\n",
    "\n",
    "print(\"\\nBest balanced-accuracy :\", best_row.to_dict())\n",
    "if not rec70_row.empty:\n",
    "    print(\"≥70 % abnormal recall :\", rec70_row.iloc[0].to_dict())\n",
    "else:\n",
    "    print(\"No threshold in scan hits 0.70 recall\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eada00aa-616c-466b-b499-eaa8e0602a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
