{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb3aba43-4ad0-4fa9-9158-996c75ce1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, make_scorer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline          # pip install imbalanced-learn\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "DATA_DIR = Path(\".\")          # adjust if files live elsewhere\n",
    "HR_FILE  = DATA_DIR / \"heartrate_15min.csv\"\n",
    "ST_FILE  = DATA_DIR / \"minuteStepsNarrow.csv\"\n",
    "DX_FILE  = DATA_DIR / \"Diagnoses_20250404.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f01e12-f644-46f8-94c3-53366909bcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR rows: 506496 | participants: 192\n"
     ]
    }
   ],
   "source": [
    "# ---- diagnoses ---------------------------------------------\n",
    "diag = (pd.read_csv(DX_FILE, parse_dates=[\"DCDate.diagnosis_baseline\"])\n",
    "          .rename(columns={\"DCDate.diagnosis_baseline\": \"BaselineDate\"})\n",
    "          .dropna(subset=[\"BaselineDate\"])\n",
    "          [[\"PIDN\", \"BaselineDate\", \"Diagnosis_baseline_3groups\"]])\n",
    "\n",
    "# ---- heart-rate --------------------------------------------\n",
    "hr = pd.read_csv(HR_FILE, parse_dates=[\"Time\"])\n",
    "common_pidn = set(diag.PIDN) & set(hr.PIDN)\n",
    "diag = diag[diag.PIDN.isin(common_pidn)]\n",
    "hr   = hr [hr .PIDN.isin(common_pidn)]\n",
    "\n",
    "hr = hr.merge(diag[[\"PIDN\", \"BaselineDate\"]], on=\"PIDN\", how=\"left\")\n",
    "assert hr[\"BaselineDate\"].notna().all()\n",
    "print(\"HR rows:\", len(hr), \"| participants:\", hr.PIDN.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8c48d6b-701e-43a1-a327-68d291f15c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR-feature table shape: (192, 7)\n"
     ]
    }
   ],
   "source": [
    "# ---- helper: first N calendar days --------------------------\n",
    "def first_n_days(g, bdate, n=14):\n",
    "    after = g[g.Time.dt.date >= bdate]\n",
    "    start = after.Time.dt.date.min() if not after.empty else g.Time.dt.date.min()\n",
    "    end   = start + pd.Timedelta(days=n)\n",
    "    return g[(g.Time.dt.date >= start) & (g.Time.dt.date < end)]\n",
    "\n",
    "hr14 = pd.concat(\n",
    "    [first_n_days(g, g[\"BaselineDate\"].iloc[0].date(), n=14)\n",
    "     for _, g in hr.groupby(\"PIDN\")],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# ---- daily mean HR per participant (for coupling later) ----\n",
    "hr_daily = (hr14.assign(Date=hr14.Time.dt.date)\n",
    "                   .groupby([\"PIDN\", \"Date\"])[\"Value\"].mean()\n",
    "                   .rename(\"HR_daily_mean\")\n",
    "                   .reset_index())\n",
    "\n",
    "# ---- global 14-day HR stats --------------------------------\n",
    "def hr_stats(df):\n",
    "    v = df[\"Value\"].to_numpy()\n",
    "    n = v.size\n",
    "    m = lambda a: np.nan if a.size == 0 else a.mean()\n",
    "    return pd.Series({\n",
    "        \"hr_mean\"   : m(v),\n",
    "        \"hr_std\"    : np.std(v, ddof=0),\n",
    "        \"hr_min\"    : v.min(),\n",
    "        \"hr_max\"    : v.max(),\n",
    "        \"hr_iqr\"    : np.percentile(v,75)-np.percentile(v,25),\n",
    "        \"hr_rmssd\"  : np.sqrt(np.mean(np.diff(v)**2)) if n>1 else np.nan,\n",
    "    })\n",
    "\n",
    "features_hr = (hr14.groupby(\"PIDN\")[[\"Time\", \"Value\"]]\n",
    "                   .apply(hr_stats)\n",
    "                   .reset_index())\n",
    "print(\"HR-feature table shape:\", features_hr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f63ddd-223f-46a7-a168-f02f1461ff3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-feature table shape: (190, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14157\\AppData\\Local\\Temp\\ipykernel_36564\\4276235692.py:41: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(step_stats)\n",
      "C:\\Users\\14157\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2999: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\14157\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3000: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "C:\\Users\\14157\\AppData\\Local\\Temp\\ipykernel_36564\\4276235692.py:47: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda df: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "# ---- read large steps CSV in chunks & keep study PIDNs -------\n",
    "use_pids = set(features_hr.PIDN)\n",
    "chunks = []\n",
    "for chk in pd.read_csv(ST_FILE,\n",
    "                       usecols=[\"PIDN\",\"ActivityMinute\",\"Steps\"],\n",
    "                       dtype={\"PIDN\":\"int32\",\"Steps\":\"int32\"},\n",
    "                       parse_dates=[\"ActivityMinute\"],\n",
    "                       chunksize=5_000_000):\n",
    "    chunks.append(chk[chk.PIDN.isin(use_pids)])\n",
    "\n",
    "steps_raw = pd.concat(chunks, ignore_index=True).rename(\n",
    "    columns={\"ActivityMinute\":\"Time\",\"Steps\":\"Value\"}\n",
    ").merge(diag[[\"PIDN\",\"BaselineDate\"]], on=\"PIDN\", how=\"left\")\n",
    "\n",
    "# ---- slice same 14-day window --------------------------------\n",
    "step14 = pd.concat(\n",
    "    [first_n_days(g, g[\"BaselineDate\"].iloc[0].date(), n=14)\n",
    "     for _, g in steps_raw.groupby(\"PIDN\")],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# ---- daily total steps --------------------------------------\n",
    "step_daily = (step14.assign(Date=step14.Time.dt.date)\n",
    "                       .groupby([\"PIDN\",\"Date\"])[\"Value\"].sum()\n",
    "                       .rename(\"Steps_daily_total\")\n",
    "                       .reset_index())\n",
    "\n",
    "# ---- step stats across 14 days ------------------------------\n",
    "def step_stats(df):\n",
    "    v = df[\"Steps_daily_total\"].to_numpy()\n",
    "    trend = v[-1]-v[0] if len(v)>=2 else np.nan\n",
    "    return pd.Series({\n",
    "        \"steps_mean\" : v.mean(),\n",
    "        \"steps_std\"  : v.std(ddof=0),\n",
    "        \"steps_min\"  : v.min(),\n",
    "        \"steps_max\"  : v.max(),\n",
    "        \"steps_trend\": trend\n",
    "    })\n",
    "\n",
    "features_st = (step_daily.groupby(\"PIDN\")\n",
    "                        .apply(step_stats)\n",
    "                        .reset_index())\n",
    "\n",
    "# ---- HR-to-steps coupling (Pearson r across 14 days) --------\n",
    "coupling = (hr_daily.merge(step_daily, on=[\"PIDN\",\"Date\"], how=\"inner\")\n",
    "                   .groupby(\"PIDN\")\n",
    "                   .apply(lambda df: pd.Series({\n",
    "                       \"hr_steps_corr\": np.corrcoef(df.HR_daily_mean,\n",
    "                                                    df.Steps_daily_total)[0,1]\n",
    "                                             if len(df)>=3 else np.nan}))\n",
    "                   .reset_index())\n",
    "\n",
    "print(\"Step-feature table shape:\", features_st.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7be25df-83ac-4860-88e3-4db3eccd4476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged feature table saved | shape = (192, 14)\n"
     ]
    }
   ],
   "source": [
    "# merge HR stats + step stats + coupling\n",
    "features_full = (features_hr\n",
    "                 .merge(features_st, on=\"PIDN\", how=\"left\")\n",
    "                 .merge(coupling,   on=\"PIDN\", how=\"left\")\n",
    "                 .merge(diag[[\"PIDN\",\"Diagnosis_baseline_3groups\"]], on=\"PIDN\"))\n",
    "\n",
    "features_full.to_csv(\"hr_steps_14day_features.csv\", index=False)\n",
    "print(\"Merged feature table saved | shape =\", features_full.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38e9cd8d-c6f7-4b3f-844c-1215c0b5bc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 27 candidates, totalling 270 fits\n",
      "\n",
      "Best 10-fold CV balanced-accuracy: 0.308\n",
      "Best parameters: {'model__l2_regularization': 0.0, 'model__learning_rate': 0.1, 'model__max_depth': None}\n",
      "\n",
      "TEST balanced-accuracy: 0.424\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Clinically Normal       0.70      0.76      0.73        25\n",
      "     FTD syndrome       0.29      0.40      0.33         5\n",
      "           MCI/AD       0.20      0.11      0.14         9\n",
      "\n",
      "         accuracy                           0.56        39\n",
      "        macro avg       0.40      0.42      0.40        39\n",
      "     weighted avg       0.53      0.56      0.54        39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 10-fold CV  +  wider HistGB grid  (ADASYN oversampling kept)\n",
    "# ================================================================\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ---- train / test split (20 %) ---------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# ---- pipeline ---------------------------------------------------\n",
    "pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\" , StandardScaler()),\n",
    "    (\"adasyn\", ADASYN(random_state=RANDOM_STATE, sampling_strategy=\"auto\")),\n",
    "    (\"model\" , HistGradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# ---- wider grid -------------------------------------------------\n",
    "param_grid = {\n",
    "    \"model__learning_rate\"   : [0.01, 0.05, 0.1],\n",
    "    \"model__max_depth\"       : [None, 3, 5],\n",
    "    \"model__l2_regularization\": [0.0, 0.1, 0.5]\n",
    "    # add 'model__max_leaf_nodes' or 'min_samples_leaf' if desired\n",
    "}\n",
    "\n",
    "cv10 = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=cv10,\n",
    "    scoring=make_scorer(balanced_accuracy_score),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest 10-fold CV balanced-accuracy:\", round(gs.best_score_, 3))\n",
    "print(\"Best parameters:\", gs.best_params_)\n",
    "\n",
    "# ---- evaluate on held-out test set -----------------------------\n",
    "y_pred = gs.predict(X_test)\n",
    "print(\"\\nTEST balanced-accuracy:\", round(balanced_accuracy_score(y_test, y_pred), 3))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efa7639d-bffc-429a-826c-2051457c28c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "\n",
      "Best CV balanced-accuracy: 0.303\n",
      "Best params: {'model__learning_rate': 0.1, 'model__max_depth': None}\n",
      "\n",
      "TEST balanced-accuracy: 0.424\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Clinically Normal       0.70      0.76      0.73        25\n",
      "     FTD syndrome       0.29      0.40      0.33         5\n",
      "           MCI/AD       0.20      0.11      0.14         9\n",
      "\n",
      "         accuracy                           0.56        39\n",
      "        macro avg       0.40      0.42      0.40        39\n",
      "     weighted avg       0.53      0.56      0.54        39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = features_full.drop(columns=[\"Diagnosis_baseline_3groups\",\"PIDN\"])\n",
    "y = features_full[\"Diagnosis_baseline_3groups\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\" , StandardScaler()),\n",
    "    (\"adasyn\", ADASYN(random_state=RANDOM_STATE, sampling_strategy=\"auto\")),\n",
    "    (\"model\" , HistGradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"model__learning_rate\": [0.05, 0.1],\n",
    "    \"model__max_depth\"    : [None, 3]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    pipe, param_grid,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=make_scorer(balanced_accuracy_score),\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest CV balanced-accuracy:\", round(gs.best_score_,3))\n",
    "print(\"Best params:\", gs.best_params_)\n",
    "\n",
    "y_pred = gs.predict(X_test)\n",
    "print(\"\\nTEST balanced-accuracy:\", round(balanced_accuracy_score(y_test, y_pred),3))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cab77212-df81-4567-ad8f-f38659352dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance: {0: 122, 1: 70}\n",
      "Fitting 10 folds for each of 27 candidates, totalling 270 fits\n",
      "\n",
      "Best 10-fold CV balanced-accuracy: 0.588\n",
      "Best parameters: {'model__l2_regularization': 0.0, 'model__learning_rate': 0.01, 'model__max_depth': None}\n",
      "\n",
      "TEST balanced-accuracy: 0.514\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CN       0.65      0.60      0.62        25\n",
      "    Abnormal       0.38      0.43      0.40        14\n",
      "\n",
      "    accuracy                           0.54        39\n",
      "   macro avg       0.51      0.51      0.51        39\n",
      "weighted avg       0.55      0.54      0.54        39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Binary screen: Clinically Normal (0) vs Abnormal (1)\n",
    "# Features: 14-day HR statistics + daily-step statistics + HR↔steps coupling\n",
    "# ================================================================\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, make_scorer\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ----- 1.  X / y (binary) -------------------------------------\n",
    "X = features_full.drop(columns=[\"Diagnosis_baseline_3groups\", \"PIDN\"])\n",
    "y = (features_full[\"Diagnosis_baseline_3groups\"] != \"Clinically Normal\").astype(int)\n",
    "\n",
    "print(\"Class balance:\", y.value_counts().to_dict())  # sanity-check\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# ----- 2.  pipeline (no ADASYN) -------------------------------\n",
    "pipe_bin = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\" , StandardScaler()),\n",
    "    (\"model\" , HistGradientBoostingClassifier(\n",
    "                  random_state=RANDOM_STATE,\n",
    "                  class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"model__learning_rate\"   : [0.01, 0.05, 0.1],\n",
    "    \"model__max_depth\"       : [None, 3, 5],\n",
    "    \"model__l2_regularization\": [0.0, 0.1, 0.5],\n",
    "}\n",
    "\n",
    "cv10 = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    pipe_bin,\n",
    "    param_grid,\n",
    "    cv=cv10,\n",
    "    scoring=make_scorer(balanced_accuracy_score),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest 10-fold CV balanced-accuracy:\", round(gs.best_score_, 3))\n",
    "print(\"Best parameters:\", gs.best_params_)\n",
    "\n",
    "# ----- 3.  evaluate on held-out test set -----------------------\n",
    "y_pred = gs.predict(X_test)\n",
    "print(\"\\nTEST balanced-accuracy:\", round(balanced_accuracy_score(y_test, y_pred), 3))\n",
    "print(classification_report(y_test, y_pred, target_names=[\"CN\", \"Abnormal\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "611d8672-03e3-44bf-b4b8-71912866fc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance: {0: 122, 1: 70}\n",
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n",
      "\n",
      "Best 10-fold CV balanced-accuracy: 0.63\n",
      "Best parameters: {'model__class_weight': {0: 1, 1: 2}, 'model__learning_rate': 0.01, 'model__max_depth': None, 'model__min_samples_leaf': 20}\n",
      "\n",
      "TEST balanced-accuracy: 0.546\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CN       0.68      0.52      0.59        25\n",
      "    Abnormal       0.40      0.57      0.47        14\n",
      "\n",
      "    accuracy                           0.54        39\n",
      "   macro avg       0.54      0.55      0.53        39\n",
      "weighted avg       0.58      0.54      0.55        39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Binary screen: Clinically Normal (0) vs Abnormal (1)\n",
    "# Features: 14-day HR statistics + daily-step statistics + HR↔steps coupling\n",
    "# ================================================================\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, make_scorer\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ----- 1.  X / y (binary) -------------------------------------\n",
    "X = features_full.drop(columns=[\"Diagnosis_baseline_3groups\", \"PIDN\"])\n",
    "y = (features_full[\"Diagnosis_baseline_3groups\"] != \"Clinically Normal\").astype(int)\n",
    "\n",
    "print(\"Class balance:\", y.value_counts().to_dict())  # sanity-check\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# ----- 2.  pipeline (no ADASYN) -------------------------------\n",
    "pipe_bin = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\" , StandardScaler()),\n",
    "    (\"model\" , HistGradientBoostingClassifier(\n",
    "                  random_state=RANDOM_STATE,\n",
    "                  class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"model__learning_rate\"   : [0.01, 0.05],\n",
    "    \"model__max_depth\"       : [None, 5],\n",
    "    \"model__min_samples_leaf\": [10, 20, 30],\n",
    "    \"model__class_weight\"    : [{0:1, 1:2}, {0:1, 1:3}]\n",
    "}\n",
    "\n",
    "\n",
    "cv10 = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    pipe_bin,\n",
    "    param_grid,\n",
    "    cv=cv10,\n",
    "    scoring=make_scorer(balanced_accuracy_score),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest 10-fold CV balanced-accuracy:\", round(gs.best_score_, 3))\n",
    "print(\"Best parameters:\", gs.best_params_)\n",
    "\n",
    "# ----- 3.  evaluate on held-out test set -----------------------\n",
    "y_pred = gs.predict(X_test)\n",
    "print(\"\\nTEST balanced-accuracy:\", round(balanced_accuracy_score(y_test, y_pred), 3))\n",
    "print(classification_report(y_test, y_pred, target_names=[\"CN\", \"Abnormal\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0d93c72-63aa-408d-9ffc-49b6f6260401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Max-BA threshold = 0.571 ---\n",
      "Balanced-acc : 0.579\n",
      "Abnormal recall : 0.357\n",
      "Confusion matrix [CN  Abn]:\n",
      " [[20  5]\n",
      " [ 9  5]]\n",
      "\n",
      "--- ≥65 % Recall threshold = 0.466 ---\n",
      "Balanced-acc : 0.577\n",
      "Abnormal recall : 0.714\n",
      "Confusion matrix [CN  Abn]:\n",
      " [[11 14]\n",
      " [ 4 10]]\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Threshold-tuning utility  (binary CN vs Abnormal)\n",
    "# ================================================================\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, balanced_accuracy_score, confusion_matrix, recall_score\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "# 1. get class-1 probabilities on the held-out test set\n",
    "probs = gs.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 2. ROC curve  → tpr (recall for class 1) / fpr / thresholds\n",
    "fpr, tpr, thr = roc_curve(y_test, probs)\n",
    "\n",
    "# 3. balanced-accuracy for every threshold\n",
    "bal_acc = []\n",
    "for t in thr:\n",
    "    y_pred = (probs >= t).astype(int)\n",
    "    bal_acc.append(balanced_accuracy_score(y_test, y_pred))\n",
    "bal_acc = np.array(bal_acc)\n",
    "\n",
    "# 4-a. threshold that maximises balanced-accuracy\n",
    "idx_best = bal_acc.argmax()\n",
    "best_cut = thr[idx_best]\n",
    "\n",
    "# 4-b. lowest threshold that gives ≥65 % recall for Abnormal\n",
    "target_recall = 0.65\n",
    "idx_recall = np.where(tpr >= target_recall)[0]\n",
    "cut_recall = thr[idx_recall[0]] if len(idx_recall) else best_cut   # fallback\n",
    "\n",
    "# 5. evaluate both thresholds\n",
    "def eval_cut(cut, label):\n",
    "    y_pred = (probs >= cut).astype(int)\n",
    "    print(f\"\\n--- {label} threshold = {cut:.3f} ---\")\n",
    "    print(\"Balanced-acc :\", round(balanced_accuracy_score(y_test, y_pred), 3))\n",
    "    print(\"Abnormal recall :\", round(recall_score(y_test, y_pred), 3))\n",
    "    print(\"Confusion matrix [CN  Abn]:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "eval_cut(best_cut, \"Max-BA\")\n",
    "eval_cut(cut_recall, \"≥65 % Recall\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aba8bf62-e224-4d5e-b9bb-38df1e5de645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cut</th>\n",
       "      <th>bal_acc</th>\n",
       "      <th>abn_recall</th>\n",
       "      <th>false_pos</th>\n",
       "      <th>false_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.714</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.714</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.714</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.714</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.714</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.714</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.714</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.643</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.571</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.571</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.571</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.500</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.500</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.429</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.429</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.357</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.357</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.357</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.286</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.214</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.214</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cut  bal_acc  abn_recall  false_pos  false_neg\n",
       "0   0.40    0.417       0.714         22          4\n",
       "1   0.41    0.417       0.714         22          4\n",
       "2   0.42    0.437       0.714         21          4\n",
       "3   0.43    0.497       0.714         18          4\n",
       "4   0.44    0.497       0.714         18          4\n",
       "5   0.45    0.537       0.714         16          4\n",
       "6   0.46    0.577       0.714         14          4\n",
       "7   0.47    0.541       0.643         14          5\n",
       "8   0.48    0.546       0.571         12          6\n",
       "9   0.49    0.546       0.571         12          6\n",
       "10  0.50    0.546       0.571         12          6\n",
       "11  0.51    0.550       0.500         10          7\n",
       "12  0.52    0.550       0.500         10          7\n",
       "13  0.53    0.514       0.429         10          8\n",
       "14  0.54    0.514       0.429         10          8\n",
       "15  0.55    0.499       0.357          9          9\n",
       "16  0.56    0.539       0.357          7          9\n",
       "17  0.57    0.579       0.357          5          9\n",
       "18  0.58    0.543       0.286          5         10\n",
       "19  0.59    0.527       0.214          4         11\n",
       "20  0.60    0.527       0.214          4         11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best BA : {'cut': 0.57, 'bal_acc': 0.579, 'abn_recall': 0.357, 'false_pos': 5.0, 'false_neg': 9.0}\n",
      "≥65 % recall : {'cut': 0.4, 'bal_acc': 0.417, 'abn_recall': 0.714, 'false_pos': 22.0, 'false_neg': 4.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import balanced_accuracy_score, recall_score, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# predicted probs from current model\n",
    "probs = gs.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "\n",
    "results = []\n",
    "for cut in np.arange(0.40, 0.61, 0.01):\n",
    "    y_pred = (probs >= cut).astype(int)\n",
    "    ba  = balanced_accuracy_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    fp  = ((y_pred == 1) & (y_test == 0)).sum()\n",
    "    fn  = ((y_pred == 0) & (y_test == 1)).sum()\n",
    "    results.append([round(cut, 2), round(ba, 3), round(rec, 3), fp, fn])\n",
    "\n",
    "df_thr = pd.DataFrame(results,\n",
    "                      columns=[\"cut\", \"bal_acc\", \"abn_recall\", \"false_pos\", \"false_neg\"])\n",
    "display(df_thr)\n",
    "\n",
    "best_row  = df_thr.loc[df_thr[\"bal_acc\"].idxmax()]\n",
    "rec65_row = df_thr[df_thr[\"abn_recall\"] >= 0.65].head(1)\n",
    "\n",
    "print(\"\\nBest BA :\", best_row.to_dict())\n",
    "if not rec65_row.empty:\n",
    "    print(\"≥65 % recall :\", rec65_row.iloc[0].to_dict())\n",
    "else:\n",
    "    print(\"No threshold hit 0.65 recall in scanned range.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1d0256e-347b-48cb-89ad-a4b5152c3068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cut</th>\n",
       "      <th>bal_acc</th>\n",
       "      <th>abn_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cut  bal_acc  abn_recall\n",
       "0   0.40    0.474       0.429\n",
       "1   0.41    0.439       0.357\n",
       "2   0.42    0.423       0.286\n",
       "3   0.43    0.463       0.286\n",
       "4   0.44    0.447       0.214\n",
       "5   0.45    0.467       0.214\n",
       "6   0.46    0.467       0.214\n",
       "7   0.47    0.547       0.214\n",
       "8   0.48    0.547       0.214\n",
       "9   0.49    0.547       0.214\n",
       "10  0.50    0.511       0.143\n",
       "11  0.51    0.531       0.143\n",
       "12  0.52    0.531       0.143\n",
       "13  0.53    0.551       0.143\n",
       "14  0.54    0.551       0.143\n",
       "15  0.55    0.551       0.143\n",
       "16  0.56    0.551       0.143\n",
       "17  0.57    0.516       0.071\n",
       "18  0.58    0.516       0.071\n",
       "19  0.59    0.516       0.071\n",
       "20  0.60    0.516       0.071"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(ISO) Best BA : {'cut': 0.53, 'bal_acc': 0.551, 'abn_recall': 0.143}\n",
      "(ISO) ≥65 % recall : none\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import balanced_accuracy_score, recall_score, confusion_matrix\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# 1️⃣  wrap the trained pipeline in an isotonic calibrator (5-fold CV on training data)\n",
    "cal_model = CalibratedClassifierCV(\n",
    "    estimator=gs.best_estimator_,     # ← changed keyword\n",
    "    method=\"isotonic\",\n",
    "    cv=5\n",
    ")\n",
    "cal_model.fit(X_train, y_train)\n",
    "\n",
    "# 2️⃣  calibrated probabilities on the test set\n",
    "probs_cal = cal_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 3️⃣  scan thresholds 0.40-0.60 by 0.01\n",
    "records = []\n",
    "for cut in np.arange(0.40, 0.61, 0.01):\n",
    "    y_pred = (probs_cal >= cut).astype(int)\n",
    "    ba  = balanced_accuracy_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    records.append([round(cut,2), round(ba,3), round(rec,3)])\n",
    "\n",
    "df_cal = pd.DataFrame(records, columns=[\"cut\",\"bal_acc\",\"abn_recall\"])\n",
    "display(df_cal)\n",
    "\n",
    "best_row  = df_cal.loc[df_cal[\"bal_acc\"].idxmax()]\n",
    "rec65_row = df_cal[df_cal[\"abn_recall\"] >= 0.65].head(1)\n",
    "\n",
    "print(\"\\n(ISO) Best BA :\", best_row.to_dict())\n",
    "print(\"(ISO) ≥65 % recall :\", rec65_row.iloc[0].to_dict() if not rec65_row.empty else \"none\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff22c3d-a190-4281-943e-b083ba96f966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
