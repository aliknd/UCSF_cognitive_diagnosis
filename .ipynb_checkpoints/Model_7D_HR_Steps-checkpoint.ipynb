{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3aba43-4ad0-4fa9-9158-996c75ce1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 0. Imports & paths =========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# modelling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score, classification_report\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from imblearn.pipeline import Pipeline        # pip install imbalanced-learn\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "DATA_DIR = Path(\"./\")                         # adjust if needed\n",
    "HR_FILE  = DATA_DIR / \"heartrate_15min.csv\"\n",
    "DX_FILE  = DATA_DIR / \"Diagnoses_20250404.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f01e12-f644-46f8-94c3-53366909bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1. Load tables & attach BaselineDate ======================\n",
    "# --- diagnoses --------------------------------------------------------\n",
    "diag = (pd.read_csv(DX_FILE, parse_dates=[\"DCDate.diagnosis_baseline\"])\n",
    "          .rename(columns={\"DCDate.diagnosis_baseline\": \"BaselineDate\"}))\n",
    "\n",
    "# keep only rows that HAVE a baseline date\n",
    "diag = diag.dropna(subset=[\"BaselineDate\"])\n",
    "diag = diag[[\"PIDN\", \"BaselineDate\", \"Diagnosis_baseline_3groups\"]]\n",
    "\n",
    "# --- heart-rate -------------------------------------------------------\n",
    "hr = pd.read_csv(HR_FILE, parse_dates=[\"Time\"])\n",
    "\n",
    "# --- intersect on PIDN -----------------------------------------------\n",
    "common_pidn = set(diag.PIDN) & set(hr.PIDN)\n",
    "diag = diag[diag.PIDN.isin(common_pidn)].copy()\n",
    "hr   = hr [hr .PIDN.isin(common_pidn)].copy()\n",
    "\n",
    "# attach each participant's baseline date to every HR row\n",
    "hr = hr.merge(diag[[\"PIDN\", \"BaselineDate\"]], on=\"PIDN\", how=\"left\")\n",
    "assert hr[\"BaselineDate\"].notna().all()\n",
    "\n",
    "print(\"HR rows:\", len(hr), \"| participants:\", hr.PIDN.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c48d6b-701e-43a1-a327-68d291f15c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2. Build the 7-day HR window ==============================\n",
    "def first_n_days(grp: pd.DataFrame, baseline_date, n=7):\n",
    "    \"\"\"Return rows for the first n calendar days on/after baseline_date.\"\"\"\n",
    "    after = grp[grp.Time.dt.date >= baseline_date]\n",
    "    start = after.Time.dt.date.min() if not after.empty else grp.Time.dt.date.min()\n",
    "    end   = start + pd.Timedelta(days=n)        # exclusive upper bound\n",
    "    return grp[(grp.Time.dt.date >= start) & (grp.Time.dt.date < end)]\n",
    "\n",
    "hr7_slices = []\n",
    "for pid, g in hr.groupby(\"PIDN\"):\n",
    "    bdate = g[\"BaselineDate\"].iloc[0].date()\n",
    "    win   = first_n_days(g, bdate, n=7)\n",
    "    if not win.empty:\n",
    "        hr7_slices.append(win)\n",
    "\n",
    "hr7 = pd.concat(hr7_slices, ignore_index=True)\n",
    "print(\"7-day HR rows:\", len(hr7), \"| participants:\", hr7.PIDN.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f63ddd-223f-46a7-a168-f02f1461ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- STEP 3 : 7-day HR feature table ----------------------\n",
    "# (run after Steps 1–2 so `hr7` and `diag` are defined)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def hr_features(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Summary stats across the participant’s 7-day HR window.\"\"\"\n",
    "    v = df[\"Value\"].to_numpy()\n",
    "    n = v.size\n",
    "    hrs        = df[\"Time\"].dt.hour\n",
    "    day_mask   = hrs.between(6, 21)\n",
    "    night_mask = ~day_mask\n",
    "    mean_ = lambda a: np.nan if a.size == 0 else a.mean()\n",
    "    pct_  = lambda cond: np.nan if n == 0 else cond.mean()\n",
    "\n",
    "    return pd.Series({\n",
    "        \"hr_mean\"   : mean_(v),\n",
    "        \"hr_median\" : np.nan if n == 0 else np.median(v),\n",
    "        \"hr_std\"    : np.nan if n == 0 else np.std(v, ddof=0),\n",
    "        \"hr_min\"    : np.nan if n == 0 else v.min(),\n",
    "        \"hr_max\"    : np.nan if n == 0 else v.max(),\n",
    "        \"hr_iqr\"    : np.nan if n == 0 else np.percentile(v, 75) - np.percentile(v, 25),\n",
    "        \"hr_p10\"    : np.nan if n == 0 else np.percentile(v, 10),\n",
    "        \"hr_p90\"    : np.nan if n == 0 else np.percentile(v, 90),\n",
    "        \"tachy_prop\": pct_(v > 100),\n",
    "        \"rmssd\"     : np.nan if n < 2 else np.sqrt(np.mean(np.diff(v)**2)),\n",
    "        \"day_mean\"  : mean_(df.loc[day_mask,   \"Value\"].to_numpy()),\n",
    "        \"night_mean\": mean_(df.loc[night_mask, \"Value\"].to_numpy())\n",
    "    })\n",
    "\n",
    "# --- build one row per participant ------------------------------------\n",
    "features7 = (\n",
    "    hr7.groupby(\"PIDN\")[[\"Time\", \"Value\"]]     # only the cols we need → no warning\n",
    "       .apply(hr_features)                    # apply summariser\n",
    "       .reset_index()                         # PIDN becomes a normal column\n",
    ")\n",
    "\n",
    "# drop any duplicated columns that might sneak in\n",
    "features7 = features7.loc[:, ~features7.columns.duplicated()]\n",
    "\n",
    "# keep numeric feature columns + PIDN\n",
    "numeric_cols = features7.select_dtypes(include=\"number\").columns.difference([\"PIDN\"])\n",
    "features7    = features7[[\"PIDN\"] + numeric_cols.tolist()]\n",
    "\n",
    "# attach 3-group diagnosis\n",
    "data7 = features7.merge(\n",
    "    diag[[\"PIDN\", \"Diagnosis_baseline_3groups\"]],\n",
    "    on=\"PIDN\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# save for convenience\n",
    "data7.to_csv(\"hr7day_features.csv\", index=False)\n",
    "print(\"Saved hr7day_features.csv | shape =\", data7.shape)\n",
    "\n",
    "# quick look\n",
    "data7.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be25df-83ac-4860-88e3-4db3eccd4476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Binary sanity-check:  Clinically Normal (0)  vs  Abnormal (1)\n",
    "# ================================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ----- 1.  X / y  -------------------------------------------------\n",
    "X = data7.drop(columns=[\"Diagnosis_baseline_3groups\", \"PIDN\"])\n",
    "y = (data7[\"Diagnosis_baseline_3groups\"] != \"Clinically Normal\").astype(int)   # 1 = MCI/AD or FTD\n",
    "\n",
    "print(\"Class balance:\", y.value_counts().to_dict())   # sanity-check\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# ----- 2.  pipeline (no oversampling) -----------------------------\n",
    "pipe_bin = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\" , StandardScaler()),\n",
    "    (\"model\" , HistGradientBoostingClassifier(\n",
    "                  random_state=RANDOM_STATE,\n",
    "                  class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "pipe_bin.fit(X_train, y_train)\n",
    "\n",
    "# ----- 3.  evaluation --------------------------------------------\n",
    "y_pred = pipe_bin.predict(X_test)\n",
    "print(\"\\nTEST balanced-accuracy:\",\n",
    "      round(balanced_accuracy_score(y_test, y_pred), 3))\n",
    "print(classification_report(y_test, y_pred, target_names=[\"CN\", \"Abnormal\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa7639d-bffc-429a-826c-2051457c28c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_FILE = Path(\"minuteStepsNarrow.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46b23b-578b-4450-ac43-e33342a0818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------  S-1  read & filter large steps CSV  --------------------\n",
    "use_pids = set(features7.PIDN)            # the 192 participants we kept after HR step\n",
    "\n",
    "chunks = []\n",
    "for chunk in pd.read_csv(\n",
    "        ST_FILE,\n",
    "        usecols=[\"PIDN\", \"ActivityMinute\", \"Steps\"],   # skip Fitbit model column\n",
    "        parse_dates=[\"ActivityMinute\"],\n",
    "        dtype={\"PIDN\": \"int32\", \"Steps\": \"int32\"},\n",
    "        chunksize=5_000_000,          # ~100-150 MB per chunk\n",
    "        low_memory=True):\n",
    "    \n",
    "    filt = chunk[chunk[\"PIDN\"].isin(use_pids)]\n",
    "    if not filt.empty:\n",
    "        chunks.append(filt)\n",
    "\n",
    "steps_raw = pd.concat(chunks, ignore_index=True)\n",
    "steps_raw = steps_raw.rename(columns={\"ActivityMinute\": \"Time\", \"Steps\": \"Value\"})\n",
    "print(\"Minute-steps rows kept:\", len(steps_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad847cc0-0aac-41fd-b3ee-cf42943ed01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach BaselineDate to steps rows\n",
    "steps_raw = steps_raw.merge(diag[[\"PIDN\", \"BaselineDate\"]], on=\"PIDN\", how=\"left\")\n",
    "assert steps_raw[\"BaselineDate\"].notna().all()\n",
    "\n",
    "step7 = pd.concat(\n",
    "    [first_n_days(g, g[\"BaselineDate\"].iloc[0].date(), n=7)\n",
    "     for pid, g in steps_raw.groupby(\"PIDN\") if not g.empty],\n",
    "    ignore_index=True\n",
    ")\n",
    "print(\"Minute-steps rows in 7-day window:\", len(step7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee330d3-8c4b-48bc-96b7-1d203a9c729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------  S-3  resample 15-min & build step features  -------------\n",
    "def build_step_features(grp: pd.DataFrame) -> pd.Series:\n",
    "    # 1. resample to 15-min bins, summing steps within each bin\n",
    "    ts = (grp.set_index(\"Time\")\n",
    "              .sort_index()\n",
    "              [\"Value\"]\n",
    "              .resample(\"15min\")          # ← updated keyword\n",
    "              .sum(min_count=1)\n",
    "              .ffill(limit=1))            # carry forward a single missing bin\n",
    "\n",
    "    v = ts.to_numpy()\n",
    "    n = v.size\n",
    "    hrs = ts.index.hour                  # Int64Index\n",
    "\n",
    "    # day = 06:00-21:59  (bool mask)\n",
    "    day_mask = (hrs >= 6) & (hrs <= 21)  # ← no .between() needed\n",
    "\n",
    "    mean_ = lambda a: np.nan if a.size == 0 else a.mean()\n",
    "\n",
    "    return pd.Series({\n",
    "        \"steps_mean\"      : mean_(v),\n",
    "        \"steps_std\"       : np.std(v, ddof=0),\n",
    "        \"steps_max\"       : v.max(),\n",
    "        \"steps_iqr\"       : np.percentile(v, 75) - np.percentile(v, 25),\n",
    "        \"steps_p10\"       : np.percentile(v, 10),\n",
    "        \"steps_p90\"       : np.percentile(v, 90),\n",
    "        \"sedentary_prop\"  : (v == 0).mean(),\n",
    "        \"moderate_bouts\"  : (v >= 100).sum(),   # 100+ steps / 15 min\n",
    "        \"vigorous_bouts\"  : (v >= 250).sum(),   # 250+ steps / 15 min\n",
    "        \"day_steps_mean\"  : mean_(v[day_mask]),\n",
    "        \"night_steps_mean\": mean_(v[~day_mask]),\n",
    "    })\n",
    "\n",
    "step_feat = (\n",
    "    step7.groupby(\"PIDN\")[[\"Time\", \"Value\"]]\n",
    "         .apply(build_step_features)\n",
    "         .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Step-feature table shape:\", step_feat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b287fbb-bd77-4cb3-82ff-6e9f4a801ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with existing HR feature table\n",
    "full_features = (features7\n",
    "                 .merge(step_feat, on=\"PIDN\", how=\"left\")      # may introduce NaNs\n",
    "                 .merge(diag[[\"PIDN\", \"Diagnosis_baseline_3groups\"]], on=\"PIDN\"))\n",
    "\n",
    "full_features.to_csv(\"hr_steps_7day_features.csv\", index=False)\n",
    "print(\"Merged HR+Steps feature table  |  shape =\", full_features.shape)\n",
    "\n",
    "# ---------- ready for modelling -----------------\n",
    "X = full_features.drop(columns=[\"Diagnosis_baseline_3groups\", \"PIDN\"])\n",
    "y = full_features[\"Diagnosis_baseline_3groups\"]\n",
    "\n",
    "# same pipeline you used before\n",
    "pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\" , StandardScaler()),\n",
    "    (\"adasyn\", ADASYN(random_state=RANDOM_STATE, sampling_strategy=\"auto\")),\n",
    "    (\"model\" , HistGradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"model__learning_rate\": [0.05, 0.1],\n",
    "    \"model__max_depth\"    : [None, 3]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    pipe, param_grid, cv=cv,\n",
    "    scoring=make_scorer(balanced_accuracy_score),\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "gs.fit(X, y)\n",
    "\n",
    "print(\"\\nBest CV balanced-accuracy:\", round(gs.best_score_, 3))\n",
    "print(\"Best params:\", gs.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0310c47-7896-49d1-857b-cb1738aeacaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Modelling cell – median-impute ➜ scale ➜ HistGB (no ADASYN)\n",
    "# ---------------------------------------------------------------\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, make_scorer\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- train / test split (reuse if you still have X_train, X_test) -----\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# --- pipeline without synthetic oversampling --------------------------\n",
    "pipe_no_synth = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\" , StandardScaler()),\n",
    "    (\"model\" , HistGradientBoostingClassifier(\n",
    "                  random_state=RANDOM_STATE,\n",
    "                  class_weight=\"balanced\"))      # handles imbalance internally\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"model__learning_rate\": [0.05, 0.1],\n",
    "    \"model__max_depth\"    : [None, 3]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    pipe_no_synth, param_grid, cv=cv,\n",
    "    scoring=make_scorer(balanced_accuracy_score),\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest CV balanced-accuracy:\", round(gs.best_score_, 3))\n",
    "print(\"Best parameters:\", gs.best_params_)\n",
    "\n",
    "# ---- evaluate on held-out test set -----------------------------------\n",
    "y_pred = gs.predict(X_test)\n",
    "print(\"\\nTEST balanced-accuracy:\", round(balanced_accuracy_score(y_test, y_pred), 3))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab77212-df81-4567-ad8f-f38659352dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
